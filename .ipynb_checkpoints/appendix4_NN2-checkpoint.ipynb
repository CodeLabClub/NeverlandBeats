{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **附录4：关于神经网络 2**\n",
    "\n",
    "在 1 的基础上，补充记录 **交叉熵（Cross-Entropy）** 损失函数、**Softmax** 输出层激活函数、损失函数的 **正则化（Regularization）** 、权重参数 w 的初始值设定、**超参数（hyper-parameter）** 的选择等内容，依然是基于 **[Neural Networks and Deep Learning 教程](http://neuralnetworksanddeeplearning.com/chap3.html)** 。\n",
    "\n",
    "> The philosophy is that the best entree to the plethora of available techniques is in-depth study of a few of the most important.（本教程依据的哲学是：入门当下众多技术的最好方式是深入学习其中最重要的那些。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **损失函数——交叉熵（Cross-Entropy）**\n",
    "\n",
    "在 1 中，输出层神经元使用的是 **sigmoid 激活函数**：\n",
    "\n",
    "$$a=\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "搭配 **二次损失函数（Quadratic Cost Function）**：\n",
    "\n",
    "$$C = \\frac{(y-a)^2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两者的搭配使用会造成一个问题：学习速率慢，即参数 w 和 b 相对损失函数的变化、变化很慢\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w} = (a-y)\\sigma'(z)x$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = (a-y)\\sigma'(z)$$\n",
    "\n",
    "这主要是和偏导数中的 $\\sigma'(z)$ 有关（当 $\\sigma(z)$ 趋近于 1 时，曲线平缓，即导数值 $\\sigma '(z)$ 趋近于 0）， 所以引入了 **交叉熵损失函数**，后面会看到，它与 sigmoid 激活函数的搭配使用，会回避掉 $\\sigma'(z)$\n",
    "\n",
    "$$C = - \\frac{1}{n} \\sum \\limits_x [y \\ln a + (1-y) \\ln(1-a)]$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum \\limits_x x_j (\\sigma(z)-y)$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum \\limits_x (\\sigma (z) - y)$$\n",
    "\n",
    "由上面偏导数公式可以看出，使用交叉熵损失函数后，**学习速率的变化主要取决于输出值与预期输出值间的偏差，偏差（即 $\\sigma(z)-y$）越大，学习速率越快**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **输出层激活函数——Softmax**\n",
    "\n",
    "\n",
    "$$a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出层神经元采用 Softmax 作为激活函数后，所有输出结果构成一个 **概率分布**，总和为 1。对比 sigmoid 激活函数，softmax 有两个特点：单调性，输入值 $z^L_j$ 的增大一定会使激活函数的输出值 $a^L_j$ 增大；全局性，神经元的输出值同时还受输出层其他神经元输入值的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C \\equiv - \\ln a^L_y$$\n",
    "\n",
    "关于 Softmax + Log-likelihood 与 Sigmoid + Cross-Entropy 这两对组合，参考这里的[讨论](https://stats.stackexchange.com/questions/198038/cross-entropy-or-log-likelihood-in-output-layer/445298#445298?newreg=b240ccb1a2a94674b7ce764dc80695a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **过度拟合（overfitting）与正则化（regularization）**\n",
    "\n",
    "因为神经网络模型中有大量的自由参数（权重 w 和偏差 b），所以对数据的过度拟合会是一个重要问题，过度拟合指的是描述训练数据的模型没有抓到本质，而把训练数据集中因为各种随机因素出现的浮动或偏差等全都考虑在内进行拟合了，结果就是，对训练数据完美拟合，损失函数值很小，但是得到的模型拿到测验数据中测试时的表现并不理想。应对这个问题的方法有：\n",
    "\n",
    "#### **及早停止（early stopping）**\n",
    "\n",
    "使用训练数据训练神经网络的同时在 **验证数据（validation data）** 中测验（每一次训练 epoch 结束后），当测验中的表现不再有明显上升趋势的时候即停止训练\n",
    "\n",
    "关于验证数据的作用，会使用他们来为神经网络的构建确定合适的超参数（hyper-parameter)，然后在测验数据中测试效果，如果使用测验数据选择则可能导致这些参数会对测验数据过度拟合，所以独立于训练数据和测验数据的验证数据很有必要。\n",
    "\n",
    "\n",
    "#### **增加训练数据量**\n",
    "\n",
    "\n",
    "#### **正则化——L2 正则**\n",
    "\n",
    "思路是在损失函数中增加一个正则项：\n",
    "\n",
    "$$C = -\\frac{1}{n}\\sum\\limits_{x_j}[y_j\\ln a^L_j + (1-y_j)\\ln(1-a^L_j)]+\\frac{\\lambda}{2n}\\sum\\limits_w w^2$$\n",
    "\n",
    "以上公式就是在交叉熵损失函数的末尾，增加了正则项，$\\lambda$ 是正则参数，可以看出正则项与 w 有关，与 b 无关。表达的意思就是要在损失与权重值 w 之间取平衡，如果正则参数值大，偏重要小的参数值，如果正则参数小，就是偏重损失小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_0$ 表示未加正则项的损失函数，损失函数加入正则项， 对反向传播的影响只是 w 相对 C 的变化多了一项 $\\frac{\\lambda}{n} w$，也好计算，w 和 b 的更新与前面一样：\n",
    "\n",
    "$$b - \\eta\\frac{\\partial C_0}{\\partial b}$$\n",
    "$$w - \\eta\\frac{\\partial C_0}{\\partial w} - \\frac{\\eta \\lambda}{n} w = (1 - \\frac{\\eta \\lambda}{n}) w - \\eta \\frac{\\partial C_0}{\\partial w}$$\n",
    "\n",
    "所以，b 同前，w 也同前，只是按 $1-\\frac{\\eta \\lambda}{n}$ 对它做了收缩，也称**权重衰减（weight decay）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  In a nutshell, regularized networks are constrained to build relatively simple models based on patterns seen often in the training data, and are resistant to learning peculiarities of the noise in the training data. The hope is that this will force our networks to do real learning about the phenomenon at hand, and to generalize better from what they learn.\n",
    "\n",
    "为什么在损失函数中加入正则项会有助于避免过度拟合的问题？因为正则项会约束神经网络习得较小的权重 w，这就意味着，输入数据的变化对输出数据的影响是相对微弱的，神经元们不用对某些具体的误差/噪声大惊小怪，而是要在训练数据集中识别相对稳定的模式（pattern）。较小的参数，意味着更简单的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "#### **正则化——L1 正则**\n",
    "\n",
    "$$C = C_0 + \\frac{\\lambda}{n} \\sum \\limits_w |w|$$\n",
    "\n",
    "L1 与 L2 很相似，从公式可以看出，L1 使用的是 |w|，所以求导后这部分是一个常数，而 L2 是二次项，求导后含有一次项，所以区别就是 L2 对 w 的收缩受 w 本身数值的影响，而 L1 不受（是常数）。于是，如果 w 较大，L2 的收缩程度更大，如果 w 较小，则 L1 的收缩程度更大：\n",
    "\n",
    "反向传播中 w 的更新变化，使用 L1：\n",
    "\n",
    "$$w-\\frac{\\eta \\lambda}{n}sgn(w) -\\eta \\frac{\\partial C_0}{\\partial w}$$\n",
    "\n",
    "$sng(w)$ 表示 w 的正负，若为正取 1，若为负取 -1，总之这部分是一个常数，与 w 的数值大小无关，下面 L2 则不同\n",
    "\n",
    "反向传播中 w 的更新变化，使用 L2：\n",
    "\n",
    "$$w  - \\frac{\\eta \\lambda}{n} w - \\eta\\frac{\\partial C_0}{\\partial w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **正则化——dropout**\n",
    "\n",
    "不改变损失函数，而是调整神经网络的结构，每一个 mini-batch 都 **随机** 使用隐藏层的一半神经元\n",
    "\n",
    "> \"This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\" In other words, if we think of our network as a model which is making predictions, then we can think of dropout as a way of making sure that the model is robust to the loss of any individual piece of evidence. \n",
    "\n",
    "很有意思，就是要限制神经元之间的依赖，使他们尽可能独立稳定地做出稳健的判断\n",
    "\n",
    "&emsp;\n",
    "\n",
    "#### **正则化——人为扩充训练数据量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **权重初始化**\n",
    "\n",
    "在 1 中，参数权重 w 的初始值是从平均数为 0、标准差为 1 的正态高斯分布中随机抽取的，这样做存在的问题是由此得到的 z 值（也符合正态分布）标准差过大，z 值的取值范围过大近而使得 sigmoid 激活函数取值趋近于 0 或 1，即神经元饱和，由此导致学习速率很慢，即权重的改变对输出结果的改变微乎其微，最终对损失函数的影响也是非常微弱，这与前面分析 sigmoid 作为输出层激活函数不足的原因相似。所以，相比 0 为均数、1 为标准差，更合理的初始值选取是从 **0 为均值、 $\\frac{1}{\\sqrt{n_{in}}}$ 为标准差** 的正态分布中随机抽取，也就是收缩高斯分布的范围，避免神经元饱和。\n",
    "\n",
    "偏差 b 初始值选取还如前，从平均数为 0、标准差为 1 的正态分布中随机抽取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "&emsp;\n",
    "\n",
    "## **改进后的神经网络**\n",
    "\n",
    "相比 1，将损失函数替换为交叉熵（Cross-Entropy）、增加正则项、更改权重参数 w 初始值选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"\n",
    "        如果 a 与 y 都取 1，np.nan_to_num 保证返回值 nan 被转为 0.0\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"\n",
    "        z 在这里并没有被使用，依然把它作为参数放进来是为了和其他损失函数的 delta 方法的输入参数保持一致\n",
    "        \"\"\"\n",
    "        return (a-y)\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()      # 更换了参数 w 的初始值选取方法\n",
    "        self.cost=cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda=0.0, evaluation_data=None, \n",
    "            monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
    "           monitor_training_accuracy=False):\n",
    "        \"\"\"\n",
    "        evaluation 通常用验证或测验数据，可以 monitor cost 或 accuracy。监测都是在每个 epoch 之后，并返回一个元组，\n",
    "        元组内包含四个元素，分别是（每个 epoch）测验数据的损失、判别准确率、训练数据的损失、判别准确率\n",
    "        \"\"\"\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch %s training complete\" % j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {} \".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"\n",
    "        以一个 mini_batch 为单位，采用梯度下降算法，通过反向传播更新权重 w 和偏差 b。mini_batch 是一个由元组构成的列表，\n",
    "        元组中存放训练数据(x,y)，eta 是学习速率，lmbda 是正则参数，n 是全部训练数据数\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w - (eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-eta(len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        返回一个元组(nabla_b, nabla_w)，表示损失函数 C_x 的梯度\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # 正向传播\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # 反向传播\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # 下面部分和 1 略有不同，l = 1 代表最后一层神经元，2 表示倒数第 2 层神经元\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta)*sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.delta(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def accuracy(self, data, conver=False):\n",
    "        \"\"\"\n",
    "        返回的是输入数据中，神经网络判别正确的数量\n",
    "        \n",
    "        如果是验证或测验数据，convert 设为 False，如果是训练数据，设为 True，主要是因为不同数据集内 y 的表征不同，\n",
    "        而使用不同的表征方式主要是了计算效率\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
    "            \n",
    "        return sum(int(x==y) for (x, y) in results)\n",
    "    \n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"\n",
    "        如果是训练数据，convert 设为 False；测验或验证数据，设为 True\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(np.linglg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "    \n",
    "    def save(self, filename):\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\":[w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "        \n",
    "    def load(filename):\n",
    "        f = open(filename, \"r\")\n",
    "        data = json.load(f)\n",
    "        f.close\n",
    "        cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "        net = Network(data[\"sizes\"], cost=cost)\n",
    "        net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "        net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "        return net\n",
    "    \n",
    "    def vectorized_result(j):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
