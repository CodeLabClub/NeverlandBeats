{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **附录3：关于循环神经网络（Recurrent Neural Network, RNN）**\n",
    "\n",
    "参考[这篇教程](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)（注：原教程代码是 python2，下面已替换为 3）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比 NN， RNN 的关键点是对 **序列信息（sequential information)** 的使用（如 madmom 就是用 RNN 模型分析音频数据，而音乐简直就是时间的艺术）\n",
    "\n",
    "也可以理解为 RNN 是有记忆（memory）的，即隐藏状态（hidden state），它记得前面已经完成的计算\n",
    "\n",
    "在 NN 中，每一层激活函数的参数都是不同的，而在 RNN 中，每一层的参数都是相同的，不同的是输入在变，所以 RNN 是对序列输入重复执行相同的任务\n",
    "\n",
    "最常用的 RNN 是 LSTM（long short-term memory），其他还有如双向循环神经网络（Bidirectional RNN）以及深度（双向）循环神经网络（Deep (Bidirectional) RNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **使用 RNN 构建一个语言模型（Language Model）**\n",
    "\n",
    "这个语言模型可以用来预测由数个单词构成的这个句子的概率，这样的模型可以应作评分机制，如机器翻译输出中哪一个备选句子更可能是一个合语法的真句子，或是语音识别中，哪一个句子更符合真实所说的话等。此外，这个模型其实还是一个生成性（generative）模型，也就是说可以根据前面序列中的单词预测下一个要出现的单词是什么（哪一个概率最高），可以参看[这篇博文](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)。\n",
    "\n",
    "### **训练数据与预处理**\n",
    "\n",
    "作者选用的训练数据是美国 Reddit 网站上的 15000 条评论。\n",
    "\n",
    "预处理过程：\n",
    "\n",
    "1. 分词（Tokenize Text）：使用 [NLTK](https://github.com/nltk/nltk) python 库 \n",
    "\n",
    "2. 剔除低频词\n",
    "\n",
    "3. 预留句子开始和结束 token \n",
    "\n",
    "4. 创建训练数据矩阵：RNN 的输入是向量而不是字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "切分 79170 个句子。\n",
      "找到 63024 个无重复词语\n",
      "词汇量是 8000 个。\n",
      "最低频的词语是 'appointments'，出现了 10 次。\n",
      "\n",
      " 示例句子：'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      " 经预处理后的示例句子：'['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 8000   # 不在此列的单词都会作为低频词剔除，用 UNKNOWN_TOKEN 代替\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "\n",
    "# 读取数据，预留句子起始与结束 token\n",
    "with open('data/reddit-comments-2015-08.csv','r') as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    next(reader)\n",
    "    # 将全部评论分割成句子\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # \n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "\n",
    "print(\"切分 %d 个句子。\" % (len(sentences)))\n",
    "\n",
    "\n",
    "# 将句子分为词语\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "# 记数词频\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print(\"找到 %d 个无重复词语\" % word_freq.B())\n",
    "\n",
    "\n",
    "# 选取最高频的词语，构建索引到词和词到索引的向量\n",
    "vocab = word_freq.most_common(vocabulary_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print(\"词汇量是 %d 个。\" % vocabulary_size)\n",
    "print(\"最低频的词语是 '%s'，出现了 %d 次。\" % (vocab[-1][0], vocab[-1][-1]))\n",
    "\n",
    "\n",
    "# 用 unknown_token 替换所有不在我们词汇表（8000 个）中的词语\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    \n",
    "print(\"\\n 示例句子：'%s'\" % sentences[0])\n",
    "print(\"\\n 经预处理后的示例句子：'%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练数据\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_STARTwhataren'tyouunderstandingaboutthis?!\n",
      "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
      "ny:\n",
      "whataren'tyouunderstandingaboutthis?!SENTENCE_END\n",
      "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n"
     ]
    }
   ],
   "source": [
    "# 打印一组训练数据看看\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print(\"x:\\n%s\\n%s\" % (\"\".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print(\"ny:\\n%s\\n%s\" % (\"\".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "### **构建 RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**初始化**\n",
    "\n",
    "首先定义一个 RNN 类来初始化模型参数。\n",
    "\n",
    "不能将模型参数的初始值设为 0，应该是接近于 0 的随机值。因为初始值的设置会影响训练结果，所以这是一个值得研究的问题，不能大意，但这里也不多讲。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # word_dim 是词汇表的容量（8000），hidden_dim 是隐藏层的维度\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        # 参数初始化，np.random.uniform 函数内的参数分别对应最小值、最大值、矩阵形状/size\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "**正向传播（Forward Propagation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z)/((np.exp(z)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # 总的时间步数\n",
    "    T = len(x)\n",
    "    # 将正向传播过程中所有的隐藏状态保存在 s 中，并额外添加一个最开始的状态，设为 0 \n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # 每一步的输出结果存在 o 中\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    for t in np.arange(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # 运行正向传播，返回概率值最大的输出结果\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 8000)\n",
      "[[0.00012491 0.00012538 0.00012504 ... 0.00012525 0.00012517 0.00012563]\n",
      " [0.00012502 0.00012472 0.00012599 ... 0.00012518 0.00012481 0.0001244 ]\n",
      " [0.00012412 0.00012515 0.00012505 ... 0.00012376 0.00012535 0.00012589]\n",
      " ...\n",
      " [0.00012507 0.00012542 0.00012463 ... 0.00012505 0.00012638 0.00012574]\n",
      " [0.0001249  0.0001255  0.0001248  ... 0.00012524 0.00012543 0.00012465]\n",
      " [0.00012604 0.00012351 0.00012558 ... 0.00012528 0.00012486 0.00012469]]\n"
     ]
    }
   ],
   "source": [
    "# 拿一句话试试看，这句话共 45 个单词，针对每一个单词都有 8000 次概率预测这个位置会出现那个单词\n",
    "# 不过当前概率值还是随机的\n",
    "\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[3316  997  712 5705 6713 2880 4673 1880 5305  865 3754 4258 1698 4335\n",
      " 3342 7614 2216 3025  338  258 6946 2315  229 2438 2437 3262 6917 5371\n",
      " 5314 5783 2536 6576  373 1109 4163 3165 4019 3115 1970  309 3669 3842\n",
      " 6211  334 5626]\n"
     ]
    }
   ],
   "source": [
    "# 返回每个位置概率值最大的单词索引值\n",
    "\n",
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "**损失函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # 针对每一句话\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        #只关心“正确的”那个单词\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # 计算 loss\n",
    "        L += -1*np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # 将全部损失除以训练样本数\n",
    "    N = sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x, y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机预测值的期待损失值：8.987197\n",
      "实际损失：8.987217\n"
     ]
    }
   ],
   "source": [
    "# 检查一下，根据随机预测值计算的损失值与理论预期基本吻合，所以模型可靠（样本量限制在1000，以便快速运算）\n",
    "print(\"随机预测值的期待损失值：%f\" % np.log(vocabulary_size))\n",
    "print(\"实际损失：%f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
