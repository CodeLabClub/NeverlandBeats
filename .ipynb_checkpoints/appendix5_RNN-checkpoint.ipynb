{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **附录 5：循环神经网络（Recurrent Neural Networks）**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面费了很大的力气研究 **神经网络**，终于走到了 **循环神经网络**，这才是 mamdom 在分析音乐节拍时使用的机器学习技术，更具体地说是循环神经网络中的 **双向长短时记忆（Bidirectional Long Short-Term Memory RNN）**。\n",
    "\n",
    "参考[附录 4.1](appendix4_NN1.ipynb) 和[附录 4.2](appendix4_NN2.ipynb)，在对 NN 有了相对清楚的理解之后（再次强烈推荐一招制敌的[**神经网络与深度学习系列教程**](http://neuralnetworksanddeeplearning.com/)），对 RNN 的理解也就很自然了。 \n",
    "\n",
    "受教程的影响，已习惯甚至偏好数学公式的表达了，所以下方图片中有很多数学符号和公式。虽然最初看起来可能确实吓人，但是公式真的能以非常精简又扎实清晰的方式阐述实质，这对于理解神经网络究竟在干嘛很有用。此外，我们其实只需要借助它们理解就可以了，实际的计算完全交给专门的软件去做，所以不用感到头疼的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "### **Vanilla RNN**\n",
    "\n",
    "(vanilla 是香草的意思，因为香草味是很基础/打底性的口味，以此命名最基本的 RNN 模型也是生动贴切）\n",
    "\n",
    "RNN 的结构与 NN 基本相同，也都是由输入层、隐藏层和输出层神经元构成，区别主要在于 RNN 的输入数据 X 在时间上有前后依赖关系。典型如音乐，正是音符前后之间的关系才构成旋律与节奏，所以节拍分析自然用的也是 RNN。如下图所示，RNN 隐藏层神经元在当前时间点上的激活状态同时还受到前一个时间点状态的影响，依次类推，当前激活状态的计算会同时纳入前面所有时间点激活状态链式的影响，下图中隐藏神经元对等号左侧 $X_T$ 的激活状态会拆解为等号右侧的部分，表示前面从 $X_{T-n}$  到 $X_{T-1}$ 所有时间点对当下时间点的影响，对应下方隐藏层激活函数公式中相比 NN 增加的部分：$W_{hh}h_{t-1}$。输出层激活函数的计算与 NN 相同，就是对隐藏层神经元的输出结果施加一个激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![vanillaRNN](img/vanillaRNN.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "### **长短时记忆（LSTM RNN）**\n",
    "\n",
    "长短时记忆也是 RNN 的一种，从下图可以看出，只是它的隐藏神经元的结构更为复杂特殊。这一方面是为了构建所谓记忆功能，即存储前面时间点上隐藏神经元的状态，同时也解决了 RNN 中如果时间点过多可能出现的梯度消失（Gradient Vanishing）或梯度爆炸（Gradient Explosion）问题。每一个隐藏神经元单元都是由 3 种门 和 1 个内部状态 4 个结构组成，具体如下：\n",
    "\n",
    "+ 遗忘门（forget gate，$f_t$）：用来控制前一个时间点隐藏层神经单元细胞状态 $C_{t-1}$ 对当前状态产生多大影响（即遗忘或者说保留多少信息）\n",
    "\n",
    "+ 输入门（input gate，$i_t$）：用来控制当前时间点候选细胞状态 $\\widetilde {C_t}$ 有多少真的被写入最终的细胞状态 $C_t$ 中\n",
    "\n",
    "+ 输出门（output gate，$o_t$）：用来控制当前时间点的细胞状态 $C_t$ 最终有多少被输出给下一层\n",
    "\n",
    "+ 细胞状态（cell state，$C_t$）：在隐藏层神经单元中随时间传递的状态\n",
    "\n",
    "&emsp;\n",
    "\n",
    "**参考：**\n",
    "\n",
    "+ [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "+ [Why are deep neural networks hard to train?](http://neuralnetworksanddeeplearning.com/chap5.html)\n",
    "\n",
    "    虽然 RNN 中隐藏层的层数可能并不多，但是因为考虑时间上的依赖关系，如上图所示，每一层的隐藏神经元可以看作是多层隐藏神经元折叠后的结果，所以也属于深度学习（deep learning）的范畴，同样面临梯度消失或梯度爆炸等问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](img/LSTM.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BiLSTM](img/BiLSTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
