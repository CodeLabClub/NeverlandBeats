{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **附录4：关于神经网络 2**\n",
    "\n",
    "在 1 的基础上，补充记录 **交叉熵（Cross-Entropy）** 损失函数、**Softmax** 输出层激活函数、损失函数的 **正则化（Regularization）** 、权重参数 w 的初始值设定、**超参数（hyper-parameter）** 的选择等内容，依然是基于 **[Neural Networks and Deep Learning 教程](http://neuralnetworksanddeeplearning.com/chap3.html)** 。\n",
    "\n",
    "> The philosophy is that the best entree to the plethora of available techniques is in-depth study of a few of the most important.（本教程依据的哲学是：入门当下众多技术的最好方式是深入学习其中最重要的那些。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **损失函数——交叉熵（Cross-Entropy）**\n",
    "\n",
    "在 1 中，输出层神经元使用的是 **sigmoid 激活函数**：\n",
    "\n",
    "$$a=\\sigma(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "搭配 **二次损失函数（Quadratic Cost Function）**：\n",
    "\n",
    "$$C = \\frac{(y-a)^2}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两者的搭配使用会造成一个问题：学习速率慢，即参数 w 和 b 相对损失函数的变化、变化很慢\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w} = (a-y)\\sigma'(z)x$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = (a-y)\\sigma'(z)$$\n",
    "\n",
    "这主要是和偏导数中的 $\\sigma'(z)$ 有关（当 $\\sigma(z)$ 趋近于 1 时，曲线平缓，即导数值 $\\sigma '(z)$ 趋近于 0）， 所以引入了 **交叉熵损失函数**，后面会看到，它与 sigmoid 激活函数的搭配使用，会回避掉 $\\sigma'(z)$\n",
    "\n",
    "$$C = - \\frac{1}{n} \\sum \\limits_x [y \\ln a + (1-y) \\ln(1-a)]$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum \\limits_x x_j (\\sigma(z)-y)$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum \\limits_x (\\sigma (z) - y)$$\n",
    "\n",
    "由上面偏导数公式可以看出，使用交叉熵损失函数后，**学习速率的变化主要取决于输出值与预期输出值间的偏差，偏差（即 $\\sigma(z)-y$）越大，学习速率越快**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;\n",
    "\n",
    "## **输出层激活函数——Softmax**\n",
    "\n",
    "\n",
    "$$a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出层神经元采用 Softmax 作为激活函数后，所有输出结果构成一个 **概率分布**，总和为 1。对比 sigmoid 激活函数，softmax 有两个特点：单调性，输入值 $z^L_j$ 的增大一定会使激活函数的输出值 $a^L_j$ 增大；全局性，神经元的输出值同时还受输出层其他神经元输入值的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C \\equiv - \\ln a^L_y$$\n",
    "\n",
    "关于 Softmax + Log-likelihood 与 Sigmoid + Cross-Entropy 这两对组合，参考这里的[讨论](https://stats.stackexchange.com/questions/198038/cross-entropy-or-log-likelihood-in-output-layer/445298#445298?newreg=b240ccb1a2a94674b7ce764dc80695a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
